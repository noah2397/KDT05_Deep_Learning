{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/noah/anaconda3/envs/torch_38/lib/python3.8/site-packages/sklearn/datasets/_openml.py:1002: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "db = fetch_openml('Fashion-MNIST', version=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "class DLDataset(TensorDataset):\n",
    "    def __init__(self, x_data, y_data):\n",
    "        super().__init__()\n",
    "        x_data = x_data.values if isinstance(x_data, pd.DataFrame) else x_data\n",
    "        y_data = y_data.values if isinstance(y_data, pd.DataFrame) else y_data\n",
    "        \n",
    "        self.feature=torch.FloatTensor(x_data)\n",
    "        self.target=torch.LongTensor(y_data)\n",
    "    def __len__(self):\n",
    "        return self.target.shape[0]\n",
    "    def __getitem__(self, index):\n",
    "        return self.feature[index], self.target[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 과제 목표 : 다중 분류 + 검증 데이터셋 사용(no_grad)\n",
    "feature = db.data # 피쳐 설정\n",
    "target = db.target # 타겟 설정 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "target = LabelEncoder().fit_transform(target)\n",
    "target=target.reshape(-1,1)\n",
    "my_dataset=DLDataset(feature, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "seed=torch.Generator().manual_seed(42)\n",
    "trainDS, validDS, testDS = random_split(my_dataset, [0.7,0.1,0.2], generator=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader \n",
    "batchsize=5\n",
    "trainDF=DataLoader(trainDS, batch_size=batchsize, shuffle=True)\n",
    "validDF=DataLoader(validDS, batch_size=batchsize, shuffle=True)\n",
    "testDF=DataLoader(testDS, batch_size=batchsize, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "EPOCHS = 10\n",
    "LOSS_FN=nn.CrossEntropyLoss().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class Model(nn.Module): # 오늘 배운 거! \n",
    "    def __init__(self):\n",
    "        super().__init__() # nn.Module 상속\n",
    "        self.fc1 = nn.Linear(784, 256) # 28 X 28 = 784 이므로, 들어가는 피쳐의 값은 784개임\n",
    "        self.fc2 = nn.Linear(256, 128) # 은닉층1\n",
    "        self.fc3 = nn.Linear(128, 64) # 은닉층2 \n",
    "        self.fc4 = nn.Linear(64, 10) # 10개의 클래스로 분류해야 하므로, 10개의 피쳐가 나오도록 설정\n",
    "        \n",
    "    def forward(self, x):  # 오늘 배운 거2!\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "model = Model() # 모델 객체 생성\n",
    "# 이제 옵티마이저와 손실함수를 설정해야한다! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=optim.Adam(model.parameters(), lr=0.001)\n",
    "import torchmetrics.functional as metrics\n",
    "\n",
    "def training():\n",
    "    loss_list=[]\n",
    "    model.train()\n",
    "    for cnt, (feature, target) in enumerate(trainDF):\n",
    "        feature, target = feature.to(DEVICE), target.to(DEVICE)\n",
    "        pre_target = model(feature.float())\n",
    "        loss=LOSS_FN(pre_target, target.squeeze())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_list.append(loss.item())\n",
    "        #print(metrics.accuracy( pre_target, target.squeeze(), task=\"multiclass\", num_classes=3 ))\n",
    "    return sum(loss_list)/len(loss_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6443451218578528"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing():\n",
    "    loss_list=[]\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        val_loss=[]\n",
    "        for cnt, (feature, target) in enumerate(validDF):\n",
    "            feature, target = feature.to(DEVICE), target.to(DEVICE)\n",
    "            pre_target = model(feature.float())\n",
    "\n",
    "            loss=LOSS_FN(pre_target, target.squeeze())\n",
    "            #print(metrics.accuracy( pre_target, target.squeeze(), task=\"multiclass\", num_classes=3 ))\n",
    "            loss_list.append(loss.item())\n",
    "    return sum(loss_list)/len(loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5784177430243082"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metrics.accuracy( pre_target, target.squeeze(), task=\"multiclass\", num_classes=3\\\n",
    "def accuracy():\n",
    "    with torch.no_grad():\n",
    "            model.eval()\n",
    "            alist=[]\n",
    "            for cnt, (feature, target) in enumerate(testDF):\n",
    "                feature, target = feature.to(DEVICE), target.to(DEVICE)\n",
    "                pre_target = model(feature.float())\n",
    "                alist.append(metrics.accuracy( pre_target, target.squeeze(), task=\"multiclass\", num_classes=10))\n",
    "    return sum(alist)/len(alist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8451)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/noah/anaconda3/envs/torch_38/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0번째 반복 ==> train_loss : 0.45, valid_loss : 0.44, 정확도 : 0.86\n",
      "1번째 반복 ==> train_loss : 0.44, valid_loss : 0.43, 정확도 : 0.85\n",
      "2번째 반복 ==> train_loss : 0.45, valid_loss : 0.45, 정확도 : 0.85\n",
      "3번째 반복 ==> train_loss : 0.43, valid_loss : 0.44, 정확도 : 0.85\n",
      "4번째 반복 ==> train_loss : 0.44, valid_loss : 0.52, 정확도 : 0.86\n",
      "5번째 반복 ==> train_loss : 0.42, valid_loss : 0.54, 정확도 : 0.84\n",
      "6번째 반복 ==> train_loss : 0.42, valid_loss : 0.51, 정확도 : 0.85\n",
      "7번째 반복 ==> train_loss : 0.43, valid_loss : 0.55, 정확도 : 0.84\n",
      "8번째 반복 ==> train_loss : 0.43, valid_loss : 0.61, 정확도 : 0.83\n",
      "9번째 반복 ==> train_loss : 0.42, valid_loss : 0.65, 정확도 : 0.84\n",
      "10번째 반복 ==> train_loss : 0.41, valid_loss : 0.57, 정확도 : 0.84\n",
      "11번째 반복 ==> train_loss : 0.42, valid_loss : 0.55, 정확도 : 0.85\n",
      "Early stopping at epoch 11\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=10, verbose=True)\n",
    "\n",
    "\n",
    "df_list=[]\n",
    "for eps in range(EPOCHS): # 50\n",
    "\n",
    "    train_loss=training()\n",
    "    valid_loss=testing()\n",
    "    accuracy1 = accuracy()\n",
    "    print(f'{eps}번째 반복 ==> train_loss : {train_loss:.2f}, valid_loss : {valid_loss:.2f}, 정확도 : {accuracy1:.2f}')\n",
    "    df_list.append([eps, train_loss, valid_loss, accuracy1])\n",
    "    scheduler.step(valid_loss)\n",
    "    if scheduler.num_bad_epochs >= scheduler.patience:\n",
    "        print(f\"Early stopping at epoch {eps}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch_PY38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
