{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mathn\\.conda\\envs\\Torch_PY38\\lib\\site-packages\\sklearn\\datasets\\_openml.py:1002: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "db = fetch_openml('Fashion-MNIST', version=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 과제 목표 : 다중 분류 + 검증 데이터셋 사용(no_grad)\n",
    "feature = db.data # 피쳐 설정\n",
    "target = db.target # 타겟 설정 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# 테스트셋 분리 : x_train, x_test, y_train, y_test\n",
    "x_train, x_test, y_train, y_test = train_test_split(feature, target, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# 표준화 스케일링\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "# 다중 분류이므로, 원핫인코딩 사용(타겟 데이터 간 상관관계가 없으므로 LabelEncoder 사용하지 않음)\n",
    "encoder = OneHotEncoder()\n",
    "# 1차원인 y_train, y_test를 2차원으로 변환 후, 원핫인코딩\n",
    "y_train = encoder.fit_transform(np.array(y_train.values).reshape(-1,1)).toarray()\n",
    "y_test = encoder.transform(np.array(y_test.values).reshape(-1,1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader # 내일 배울 거 예습! \n",
    "\n",
    "# 연산을 하기 위해서, 모든 텐서를 float텐서로 변환\n",
    "x_train = torch.FloatTensor(x_train)\n",
    "x_test = torch.FloatTensor(x_test)\n",
    "y_train = torch.FloatTensor(y_train)\n",
    "y_test = torch.FloatTensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련용 TensorDataset과 DataLoader 생성\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "# no_grad를 사용하기 위해, 테스트용 TensorDataset과 DataLoader 생성\n",
    "test_dataset = TensorDataset(x_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module): # 오늘 배운 거! \n",
    "    def __init__(self):\n",
    "        super().__init__() # nn.Module 상속\n",
    "        self.fc1 = nn.Linear(784, 256) # 28 X 28 = 784 이므로, 들어가는 피쳐의 값은 784개임\n",
    "        self.fc2 = nn.Linear(256, 128) # 은닉층1\n",
    "        self.fc3 = nn.Linear(128, 64) # 은닉층2 \n",
    "        self.fc4 = nn.Linear(64, 10) # 10개의 클래스로 분류해야 하므로, 10개의 피쳐가 나오도록 설정\n",
    "        \n",
    "    def forward(self, x):  # 오늘 배운 거2!\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "model = Model() # 모델 객체 생성\n",
    "# 이제 옵티마이저와 손실함수를 설정해야한다! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001) # Adam 옵티마이저 사용 : 오늘 배운 거3!\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "epochs = 10\n",
    "for e in range(epochs):\n",
    "    for inputs, labels in train_loader: # 데이터를 하나씩 꺼내오자...\n",
    "        \n",
    "        outputs = model(inputs) # x_train을 모델에 넣어서 예측값을 구함\n",
    "        \n",
    "        loss = loss_function(outputs, torch.max(labels, 1)[1]) # 손실함수 정의 \n",
    "        # torch.max(labels, 1)[1] : dim=1, 즉 행을 기준으로 최댓값의 인덱스를 반환\n",
    "        #=================================================\n",
    "        optimizer.zero_grad() \n",
    "        loss.backward()                      # 늘 하던 파트\n",
    "        optimizer.step()\n",
    "        #================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8895\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad(): # 검증시간, 모든 업데이트를 멈춘다! \n",
    "    total = y_test.shape[0]\n",
    "    outputs = model(x_test) # 예측값 생성\n",
    "    _, predicted = torch.max(outputs, 1) # 아까와 똑같다, 인덱스를 갖고옴, 근데 원핫인코딩 좀 별로다..일일히 변환해야 함;;\n",
    "    correct = (predicted == torch.max(y_test, 1)[1]).sum().item() # 짜다보니 이렇게 됐다...\n",
    "    print(f\"Accuracy: {correct/total}\") # 정확도 출력 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 88.9%!  생각보다 잘나왔다~"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch_PY38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
