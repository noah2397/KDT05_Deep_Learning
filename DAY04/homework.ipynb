{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "irisDF = pd.read_csv(\"./iris.csv\")\n",
    "irisNP = np.loadtxt(\"./iris.csv\",dtype=float, delimiter=',', usecols=[0, 1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DLDataset(TensorDataset):\n",
    "    def __init__(self, x_data, y_data):\n",
    "        super().__init__()\n",
    "        x_data = x_data.values if isinstance(x_data, pd.DataFrame) else x_data\n",
    "        y_data = y_data.values if isinstance(y_data, pd.DataFrame) else y_data\n",
    "        \n",
    "        self.feature=torch.FloatTensor(x_data)\n",
    "        self.target=torch.LongTensor(y_data)\n",
    "    def __len__(self):\n",
    "        return self.target.shape[0]\n",
    "    def __getitem__(self, index):\n",
    "        return self.feature[index], self.target[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = irisDF[irisDF.columns[:-1]]\n",
    "target = irisDF[irisDF.columns[-1]]\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "target = LabelEncoder().fit_transform(target)\n",
    "target=target.reshape(-1,1)\n",
    "my_dataset=DLDataset(feature, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "seed=torch.Generator().manual_seed(42)\n",
    "trainDS, validDS, testDS = random_split(my_dataset, [0.7,0.1,0.2], generator=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize=5\n",
    "trainDF=DataLoader(trainDS, batch_size=batchsize, shuffle=True)\n",
    "validDF=DataLoader(validDS, batch_size=batchsize, shuffle=True)\n",
    "testDF=DataLoader(testDS, batch_size=batchsize, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model(nn.Module) :\n",
    "    def __init__(self):\n",
    "        super(model, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 3)\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "    \n",
    "model=model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "EPOCHS = 500\n",
    "LOSS_FN=nn.CrossEntropyLoss().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=optim.Adam(model.parameters(), lr=0.001)\n",
    "import torchmetrics.functional as metrics\n",
    "\n",
    "def training():\n",
    "    loss_list=[]\n",
    "    model.train()\n",
    "    for cnt, (feature, target) in enumerate(trainDF):\n",
    "        feature, target = feature.to(DEVICE), target.to(DEVICE)\n",
    "        pre_target = model(feature.float())\n",
    "        loss=LOSS_FN(pre_target, target.squeeze())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_list.append(loss.item())\n",
    "        #print(metrics.accuracy( pre_target, target.squeeze(), task=\"multiclass\", num_classes=3 ))\n",
    "    return sum(loss_list)/len(loss_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8207704084260123"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing():\n",
    "    loss_list=[]\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        val_loss=[]\n",
    "        for cnt, (feature, target) in enumerate(validDF):\n",
    "            feature, target = feature.to(DEVICE), target.to(DEVICE)\n",
    "            pre_target = model(feature.float())\n",
    "\n",
    "            loss=LOSS_FN(pre_target, target.squeeze())\n",
    "            #print(metrics.accuracy( pre_target, target.squeeze(), task=\"multiclass\", num_classes=3 ))\n",
    "            loss_list.append(loss.item())\n",
    "    return sum(loss_list)/len(loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8215197324752808"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metrics.accuracy( pre_target, target.squeeze(), task=\"multiclass\", num_classes=3\\\n",
    "def accuracy():\n",
    "    with torch.no_grad():\n",
    "            model.eval()\n",
    "            alist=[]\n",
    "            for cnt, (feature, target) in enumerate(testDF):\n",
    "                feature, target = feature.to(DEVICE), target.to(DEVICE)\n",
    "                pre_target = model(feature.float())\n",
    "                alist.append(metrics.accuracy( pre_target, target.squeeze(), task=\"multiclass\", num_classes=3))\n",
    "    return sum(alist)/len(alist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/noah/anaconda3/envs/torch_38/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0번째 반복 ==> train_loss : 0.80, valid_loss : 0.81, 정확도 : 0.47\n",
      "10번째 반복 ==> train_loss : 0.67, valid_loss : 0.68, 정확도 : 0.48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20번째 반복 ==> train_loss : 0.59, valid_loss : 0.60, 정확도 : 0.48\n",
      "30번째 반복 ==> train_loss : 0.54, valid_loss : 0.54, 정확도 : 0.53\n",
      "40번째 반복 ==> train_loss : 0.50, valid_loss : 0.50, 정확도 : 0.55\n",
      "50번째 반복 ==> train_loss : 0.47, valid_loss : 0.47, 정확도 : 0.63\n",
      "60번째 반복 ==> train_loss : 0.45, valid_loss : 0.45, 정확도 : 0.73\n",
      "70번째 반복 ==> train_loss : 0.43, valid_loss : 0.43, 정확도 : 0.72\n",
      "80번째 반복 ==> train_loss : 0.41, valid_loss : 0.41, 정확도 : 0.75\n",
      "90번째 반복 ==> train_loss : 0.40, valid_loss : 0.40, 정확도 : 0.76\n",
      "100번째 반복 ==> train_loss : 0.39, valid_loss : 0.38, 정확도 : 0.82\n",
      "110번째 반복 ==> train_loss : 0.38, valid_loss : 0.37, 정확도 : 0.82\n",
      "120번째 반복 ==> train_loss : 0.37, valid_loss : 0.36, 정확도 : 0.82\n",
      "130번째 반복 ==> train_loss : 0.36, valid_loss : 0.35, 정확도 : 0.90\n",
      "140번째 반복 ==> train_loss : 0.35, valid_loss : 0.34, 정확도 : 0.90\n",
      "150번째 반복 ==> train_loss : 0.34, valid_loss : 0.33, 정확도 : 0.93\n",
      "160번째 반복 ==> train_loss : 0.33, valid_loss : 0.32, 정확도 : 0.93\n",
      "170번째 반복 ==> train_loss : 0.32, valid_loss : 0.31, 정확도 : 0.93\n",
      "180번째 반복 ==> train_loss : 0.32, valid_loss : 0.31, 정확도 : 0.93\n",
      "190번째 반복 ==> train_loss : 0.31, valid_loss : 0.30, 정확도 : 0.97\n",
      "200번째 반복 ==> train_loss : 0.30, valid_loss : 0.29, 정확도 : 0.97\n",
      "210번째 반복 ==> train_loss : 0.30, valid_loss : 0.29, 정확도 : 0.97\n",
      "220번째 반복 ==> train_loss : 0.29, valid_loss : 0.28, 정확도 : 0.97\n",
      "230번째 반복 ==> train_loss : 0.28, valid_loss : 0.27, 정확도 : 0.97\n",
      "240번째 반복 ==> train_loss : 0.28, valid_loss : 0.27, 정확도 : 0.97\n",
      "250번째 반복 ==> train_loss : 0.27, valid_loss : 0.26, 정확도 : 0.97\n",
      "260번째 반복 ==> train_loss : 0.27, valid_loss : 0.26, 정확도 : 0.97\n",
      "270번째 반복 ==> train_loss : 0.26, valid_loss : 0.25, 정확도 : 0.97\n",
      "280번째 반복 ==> train_loss : 0.26, valid_loss : 0.24, 정확도 : 0.97\n",
      "290번째 반복 ==> train_loss : 0.25, valid_loss : 0.24, 정확도 : 0.97\n",
      "300번째 반복 ==> train_loss : 0.25, valid_loss : 0.23, 정확도 : 1.00\n",
      "310번째 반복 ==> train_loss : 0.24, valid_loss : 0.23, 정확도 : 0.97\n",
      "320번째 반복 ==> train_loss : 0.24, valid_loss : 0.22, 정확도 : 1.00\n",
      "330번째 반복 ==> train_loss : 0.23, valid_loss : 0.22, 정확도 : 0.96\n",
      "340번째 반복 ==> train_loss : 0.23, valid_loss : 0.22, 정확도 : 1.00\n",
      "350번째 반복 ==> train_loss : 0.23, valid_loss : 0.21, 정확도 : 1.00\n",
      "360번째 반복 ==> train_loss : 0.22, valid_loss : 0.21, 정확도 : 1.00\n",
      "370번째 반복 ==> train_loss : 0.22, valid_loss : 0.20, 정확도 : 1.00\n",
      "380번째 반복 ==> train_loss : 0.22, valid_loss : 0.20, 정확도 : 1.00\n",
      "390번째 반복 ==> train_loss : 0.21, valid_loss : 0.20, 정확도 : 1.00\n",
      "400번째 반복 ==> train_loss : 0.21, valid_loss : 0.20, 정확도 : 1.00\n",
      "410번째 반복 ==> train_loss : 0.20, valid_loss : 0.19, 정확도 : 1.00\n",
      "420번째 반복 ==> train_loss : 0.20, valid_loss : 0.19, 정확도 : 1.00\n",
      "430번째 반복 ==> train_loss : 0.20, valid_loss : 0.19, 정확도 : 1.00\n",
      "440번째 반복 ==> train_loss : 0.20, valid_loss : 0.18, 정확도 : 1.00\n",
      "450번째 반복 ==> train_loss : 0.19, valid_loss : 0.18, 정확도 : 1.00\n",
      "460번째 반복 ==> train_loss : 0.19, valid_loss : 0.17, 정확도 : 1.00\n",
      "470번째 반복 ==> train_loss : 0.19, valid_loss : 0.17, 정확도 : 1.00\n",
      "480번째 반복 ==> train_loss : 0.19, valid_loss : 0.17, 정확도 : 1.00\n",
      "490번째 반복 ==> train_loss : 0.18, valid_loss : 0.17, 정확도 : 1.00\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=10, verbose=True)\n",
    "\n",
    "\n",
    "df_list=[]\n",
    "for eps in range(EPOCHS): # 50\n",
    "\n",
    "    train_loss=training()\n",
    "    valid_loss=testing()\n",
    "    accuracy1 = accuracy()\n",
    "    if eps %10 ==0 :\n",
    "        print(f'{eps}번째 반복 ==> train_loss : {train_loss:.2f}, valid_loss : {valid_loss:.2f}, 정확도 : {accuracy1:.2f}')\n",
    "        df_list.append([eps, train_loss, valid_loss, accuracy1])\n",
    "        scheduler.step(valid_loss)\n",
    "        if scheduler.num_bad_epochs >= scheduler.patience:\n",
    "            print(f\"Early stopping at epoch {eps}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.801953</td>\n",
       "      <td>0.809118</td>\n",
       "      <td>tensor(0.4750)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>0.672806</td>\n",
       "      <td>0.680043</td>\n",
       "      <td>tensor(0.4833)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>0.592195</td>\n",
       "      <td>0.596992</td>\n",
       "      <td>tensor(0.4750)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30</td>\n",
       "      <td>0.540072</td>\n",
       "      <td>0.542747</td>\n",
       "      <td>tensor(0.5333)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40</td>\n",
       "      <td>0.500255</td>\n",
       "      <td>0.500795</td>\n",
       "      <td>tensor(0.5500)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>50</td>\n",
       "      <td>0.471490</td>\n",
       "      <td>0.471149</td>\n",
       "      <td>tensor(0.6250)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>60</td>\n",
       "      <td>0.449247</td>\n",
       "      <td>0.447107</td>\n",
       "      <td>tensor(0.7250)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>70</td>\n",
       "      <td>0.430289</td>\n",
       "      <td>0.426160</td>\n",
       "      <td>tensor(0.7167)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>80</td>\n",
       "      <td>0.414190</td>\n",
       "      <td>0.409646</td>\n",
       "      <td>tensor(0.7500)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>90</td>\n",
       "      <td>0.400320</td>\n",
       "      <td>0.396723</td>\n",
       "      <td>tensor(0.7583)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>100</td>\n",
       "      <td>0.387194</td>\n",
       "      <td>0.380219</td>\n",
       "      <td>tensor(0.8250)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>110</td>\n",
       "      <td>0.376708</td>\n",
       "      <td>0.371037</td>\n",
       "      <td>tensor(0.8167)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>120</td>\n",
       "      <td>0.366212</td>\n",
       "      <td>0.359264</td>\n",
       "      <td>tensor(0.8250)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>130</td>\n",
       "      <td>0.355942</td>\n",
       "      <td>0.347781</td>\n",
       "      <td>tensor(0.9000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>140</td>\n",
       "      <td>0.346741</td>\n",
       "      <td>0.339356</td>\n",
       "      <td>tensor(0.9000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>150</td>\n",
       "      <td>0.339030</td>\n",
       "      <td>0.329050</td>\n",
       "      <td>tensor(0.9333)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>160</td>\n",
       "      <td>0.331067</td>\n",
       "      <td>0.320392</td>\n",
       "      <td>tensor(0.9333)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>170</td>\n",
       "      <td>0.322848</td>\n",
       "      <td>0.314493</td>\n",
       "      <td>tensor(0.9333)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>180</td>\n",
       "      <td>0.315228</td>\n",
       "      <td>0.307153</td>\n",
       "      <td>tensor(0.9333)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>190</td>\n",
       "      <td>0.311256</td>\n",
       "      <td>0.295147</td>\n",
       "      <td>tensor(0.9667)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>200</td>\n",
       "      <td>0.301912</td>\n",
       "      <td>0.289529</td>\n",
       "      <td>tensor(0.9667)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>210</td>\n",
       "      <td>0.295235</td>\n",
       "      <td>0.287289</td>\n",
       "      <td>tensor(0.9667)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>220</td>\n",
       "      <td>0.288955</td>\n",
       "      <td>0.279349</td>\n",
       "      <td>tensor(0.9667)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>230</td>\n",
       "      <td>0.283604</td>\n",
       "      <td>0.271932</td>\n",
       "      <td>tensor(0.9667)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>240</td>\n",
       "      <td>0.277077</td>\n",
       "      <td>0.265424</td>\n",
       "      <td>tensor(0.9667)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>250</td>\n",
       "      <td>0.271795</td>\n",
       "      <td>0.261134</td>\n",
       "      <td>tensor(0.9667)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>260</td>\n",
       "      <td>0.266560</td>\n",
       "      <td>0.255526</td>\n",
       "      <td>tensor(0.9667)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>270</td>\n",
       "      <td>0.261097</td>\n",
       "      <td>0.248423</td>\n",
       "      <td>tensor(0.9667)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>280</td>\n",
       "      <td>0.256510</td>\n",
       "      <td>0.244920</td>\n",
       "      <td>tensor(0.9667)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>290</td>\n",
       "      <td>0.252626</td>\n",
       "      <td>0.240263</td>\n",
       "      <td>tensor(0.9667)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>300</td>\n",
       "      <td>0.247916</td>\n",
       "      <td>0.232234</td>\n",
       "      <td>tensor(1.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>310</td>\n",
       "      <td>0.243359</td>\n",
       "      <td>0.229744</td>\n",
       "      <td>tensor(0.9667)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>320</td>\n",
       "      <td>0.238031</td>\n",
       "      <td>0.222528</td>\n",
       "      <td>tensor(1.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>330</td>\n",
       "      <td>0.233935</td>\n",
       "      <td>0.221402</td>\n",
       "      <td>tensor(0.9583)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>340</td>\n",
       "      <td>0.229921</td>\n",
       "      <td>0.216224</td>\n",
       "      <td>tensor(1.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>350</td>\n",
       "      <td>0.225284</td>\n",
       "      <td>0.212403</td>\n",
       "      <td>tensor(1.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>360</td>\n",
       "      <td>0.221681</td>\n",
       "      <td>0.208314</td>\n",
       "      <td>tensor(1.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>370</td>\n",
       "      <td>0.219419</td>\n",
       "      <td>0.202389</td>\n",
       "      <td>tensor(1.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>380</td>\n",
       "      <td>0.215312</td>\n",
       "      <td>0.200793</td>\n",
       "      <td>tensor(1.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>390</td>\n",
       "      <td>0.211805</td>\n",
       "      <td>0.196074</td>\n",
       "      <td>tensor(1.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>400</td>\n",
       "      <td>0.208110</td>\n",
       "      <td>0.195146</td>\n",
       "      <td>tensor(1.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>410</td>\n",
       "      <td>0.204827</td>\n",
       "      <td>0.189368</td>\n",
       "      <td>tensor(1.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>420</td>\n",
       "      <td>0.201904</td>\n",
       "      <td>0.186188</td>\n",
       "      <td>tensor(1.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>430</td>\n",
       "      <td>0.199494</td>\n",
       "      <td>0.185711</td>\n",
       "      <td>tensor(1.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>440</td>\n",
       "      <td>0.195732</td>\n",
       "      <td>0.181572</td>\n",
       "      <td>tensor(1.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>450</td>\n",
       "      <td>0.193089</td>\n",
       "      <td>0.179510</td>\n",
       "      <td>tensor(1.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>460</td>\n",
       "      <td>0.192954</td>\n",
       "      <td>0.173579</td>\n",
       "      <td>tensor(1.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>470</td>\n",
       "      <td>0.188321</td>\n",
       "      <td>0.173440</td>\n",
       "      <td>tensor(1.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>480</td>\n",
       "      <td>0.185045</td>\n",
       "      <td>0.170428</td>\n",
       "      <td>tensor(1.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>490</td>\n",
       "      <td>0.183423</td>\n",
       "      <td>0.165704</td>\n",
       "      <td>tensor(1.)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch  train_loss  valid_loss        accuracy\n",
       "0       0    0.801953    0.809118  tensor(0.4750)\n",
       "1      10    0.672806    0.680043  tensor(0.4833)\n",
       "2      20    0.592195    0.596992  tensor(0.4750)\n",
       "3      30    0.540072    0.542747  tensor(0.5333)\n",
       "4      40    0.500255    0.500795  tensor(0.5500)\n",
       "5      50    0.471490    0.471149  tensor(0.6250)\n",
       "6      60    0.449247    0.447107  tensor(0.7250)\n",
       "7      70    0.430289    0.426160  tensor(0.7167)\n",
       "8      80    0.414190    0.409646  tensor(0.7500)\n",
       "9      90    0.400320    0.396723  tensor(0.7583)\n",
       "10    100    0.387194    0.380219  tensor(0.8250)\n",
       "11    110    0.376708    0.371037  tensor(0.8167)\n",
       "12    120    0.366212    0.359264  tensor(0.8250)\n",
       "13    130    0.355942    0.347781  tensor(0.9000)\n",
       "14    140    0.346741    0.339356  tensor(0.9000)\n",
       "15    150    0.339030    0.329050  tensor(0.9333)\n",
       "16    160    0.331067    0.320392  tensor(0.9333)\n",
       "17    170    0.322848    0.314493  tensor(0.9333)\n",
       "18    180    0.315228    0.307153  tensor(0.9333)\n",
       "19    190    0.311256    0.295147  tensor(0.9667)\n",
       "20    200    0.301912    0.289529  tensor(0.9667)\n",
       "21    210    0.295235    0.287289  tensor(0.9667)\n",
       "22    220    0.288955    0.279349  tensor(0.9667)\n",
       "23    230    0.283604    0.271932  tensor(0.9667)\n",
       "24    240    0.277077    0.265424  tensor(0.9667)\n",
       "25    250    0.271795    0.261134  tensor(0.9667)\n",
       "26    260    0.266560    0.255526  tensor(0.9667)\n",
       "27    270    0.261097    0.248423  tensor(0.9667)\n",
       "28    280    0.256510    0.244920  tensor(0.9667)\n",
       "29    290    0.252626    0.240263  tensor(0.9667)\n",
       "30    300    0.247916    0.232234      tensor(1.)\n",
       "31    310    0.243359    0.229744  tensor(0.9667)\n",
       "32    320    0.238031    0.222528      tensor(1.)\n",
       "33    330    0.233935    0.221402  tensor(0.9583)\n",
       "34    340    0.229921    0.216224      tensor(1.)\n",
       "35    350    0.225284    0.212403      tensor(1.)\n",
       "36    360    0.221681    0.208314      tensor(1.)\n",
       "37    370    0.219419    0.202389      tensor(1.)\n",
       "38    380    0.215312    0.200793      tensor(1.)\n",
       "39    390    0.211805    0.196074      tensor(1.)\n",
       "40    400    0.208110    0.195146      tensor(1.)\n",
       "41    410    0.204827    0.189368      tensor(1.)\n",
       "42    420    0.201904    0.186188      tensor(1.)\n",
       "43    430    0.199494    0.185711      tensor(1.)\n",
       "44    440    0.195732    0.181572      tensor(1.)\n",
       "45    450    0.193089    0.179510      tensor(1.)\n",
       "46    460    0.192954    0.173579      tensor(1.)\n",
       "47    470    0.188321    0.173440      tensor(1.)\n",
       "48    480    0.185045    0.170428      tensor(1.)\n",
       "49    490    0.183423    0.165704      tensor(1.)"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(df_list, columns=['epoch', 'train_loss', 'valid_loss', 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch_PY38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
