{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset & DataLoader\n",
    "# -Pytorch에서 배치크기만 데이터를 조절하기 위한 메거니즘\n",
    "# -Dataset : 지정된 Dataset에서 지정된 batch size만큼 피쳐와 타겟을 추출하여 전달\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "x_data = torch.IntTensor([[10,20,30],[20,30,40],[30,40,50],[40,50,60],[50,60,70]])\n",
    "y_data = torch.IntTensor([[20],[30],[40],[50],[60]])\n",
    "\n",
    "# TensorDataset 클래스 로딩\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset    \n",
    "import torchvision\n",
    "\n",
    "Dataset = TensorDataset(x_data, y_data) # 행 번호를 맞춰야 한다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "irisDF=pd.read_csv(\"./iris.csv\")\n",
    "irisNP = np.loadtxt(\"./iris.csv\",dtype=float, delimiter=',', usecols=[0, 1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DLDataset(TensorDataset):\n",
    "    def __init__(self, x_data, y_data):\n",
    "        super().__init__()\n",
    "        x_data = x_data.values if isinstance(x_data, pd.DataFrame) else x_data # 데이터프레임이라면 값들만, 아니라면 그냥 넘겨줌 \n",
    "        y_data = y_data.values if isinstance(y_data, pd.DataFrame) else y_data\n",
    "        \n",
    "        self.feature=torch.tensor(x_data)\n",
    "        self.target=torch.tensor(y_data)\n",
    "\n",
    "    # 데이터셋의 개수 체크 함수 콜백함수\n",
    "    def __len__(self):\n",
    "        return self.target.shape[0]\n",
    "\n",
    "    # 특정 인덱스 데이터+라벨 반환 콜백함수\n",
    "    def __getitem__(self, index):\n",
    "        return self.feature[index], self.target[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(irisDF)  # 타입 확인하는 방법 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF\n"
     ]
    }
   ],
   "source": [
    "if irisDF.__class__.__name__ == 'DataFrame': # 타입 확인하는 방법 \n",
    "    print(\"DF\")\n",
    "else :\n",
    "    print(\"----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, False)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(irisDF, pd.DataFrame), isinstance(irisNP, pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance([10],list), isinstance({'A':22}, dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = irisDF[irisDF.columns[:-1]]\n",
    "target = irisDF[irisDF.columns[-1]]\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "target = LabelEncoder().fit_transform(target)\n",
    "target=target.reshape(-1,1)\n",
    "\n",
    "my_dataset=DLDataset(feature, target) # 피쳐와 라벨로 분리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([4.9000, 3.0000, 1.4000, 0.2000], dtype=torch.float64), tensor([0]))"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataset2 = DLDataset(irisNP, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([5.1000, 3.5000, 1.4000, 0.2000], dtype=torch.float64), tensor([0]))"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dataset2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainDF => 105, validDF => 15개, testDF => 29\n",
      "Subset 속성 => [17, 100, 111, 49, 30, 124, 119, 55, 72, 142, 129, 93, 86, 103, 140, 13, 64, 108, 91, 25, 135, 73, 31, 82, 48, 106, 40, 44, 130, 24, 67, 3, 102, 54, 137, 63, 71, 61, 75, 59, 141, 107, 50, 112, 36, 8, 26, 11, 126, 29, 41, 74, 96, 128, 1, 33, 20, 105, 52, 42, 2, 39, 109, 62, 10, 28, 87, 121, 16, 65, 122, 104, 15, 69, 147, 84, 114, 58, 45, 110, 143, 144, 113, 27, 116, 6, 120, 18, 19, 32, 145, 134, 146, 98, 79, 9, 132, 5, 133, 23, 47, 66, 94, 136, 22]\n",
      "Subset 속성 => <__main__.DLDataset object at 0x7faac67cba30>\n"
     ]
    }
   ],
   "source": [
    "# 학습용, 검증용, 테스트용 Dataset\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# 학습용, 검증용, 테스트 데이터 비율\n",
    "# 갯수로 주던가, 비율로 주던가 할 수 있다!\n",
    "seed=torch.Generator().manual_seed(42)\n",
    "trainDS, validDS, testDS = random_split(my_dataset, [0.7,0.1,0.2], generator=seed)\n",
    "\n",
    "print(f\"trainDF => {len(trainDS)}, validDF => {len(validDS)}개, testDF => {len(testDS)}\")\n",
    "print(f\"Subset 속성 => {trainDS.indices}\") # 인덱스를 보여준다!\n",
    "print(f\"Subset 속성 => {trainDS.dataset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21, 3, 6)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batchsize=5\n",
    "# drop_last 매개변수 : 배치사이즈로 데이터셋 분리 후 남는 데이터 처리 방법 설정 [기본 : False ]\n",
    "trainDF=DataLoader(trainDS, batch_size=batchsize, shuffle=True)\n",
    "validDF=DataLoader(validDS, batch_size=batchsize, shuffle=True)\n",
    "testDF=DataLoader(testDS, batch_size=batchsize, shuffle=True)\n",
    "\n",
    "# Epoch당 꺼내는 횟수 \n",
    "len(trainDF), len(validDF), len(testDF) # 105/5=21, 15/5=3. 29/5=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.Subset at 0x7faad6333310>"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] feature tensor([[5.8000, 2.8000, 5.1000, 2.4000],\n",
      "        [4.8000, 3.4000, 1.6000, 0.2000],\n",
      "        [4.4000, 3.2000, 1.3000, 0.2000],\n",
      "        [4.6000, 3.1000, 1.5000, 0.2000],\n",
      "        [5.3000, 3.7000, 1.5000, 0.2000]], dtype=torch.float64)\n",
      "[1] feature tensor([[6.5000, 3.0000, 5.2000, 2.0000],\n",
      "        [6.1000, 2.6000, 5.6000, 1.4000],\n",
      "        [5.4000, 3.4000, 1.7000, 0.2000],\n",
      "        [5.9000, 3.2000, 4.8000, 1.8000],\n",
      "        [5.4000, 3.4000, 1.5000, 0.4000]], dtype=torch.float64)\n",
      "[2] feature tensor([[5.8000, 2.7000, 5.1000, 1.9000],\n",
      "        [5.6000, 3.0000, 4.1000, 1.3000],\n",
      "        [6.8000, 3.0000, 5.5000, 2.1000],\n",
      "        [5.2000, 3.4000, 1.4000, 0.2000],\n",
      "        [6.4000, 3.2000, 4.5000, 1.5000]], dtype=torch.float64)\n",
      "[3] feature tensor([[6.0000, 2.2000, 4.0000, 1.0000],\n",
      "        [5.8000, 2.6000, 4.0000, 1.2000],\n",
      "        [7.4000, 2.8000, 6.1000, 1.9000],\n",
      "        [6.3000, 2.3000, 4.4000, 1.3000],\n",
      "        [4.9000, 2.5000, 4.5000, 1.7000]], dtype=torch.float64)\n",
      "[4] feature tensor([[6.3000, 3.3000, 4.7000, 1.6000],\n",
      "        [4.8000, 3.0000, 1.4000, 0.3000],\n",
      "        [4.8000, 3.4000, 1.9000, 0.2000],\n",
      "        [7.7000, 3.8000, 6.7000, 2.2000],\n",
      "        [6.3000, 2.5000, 5.0000, 1.9000]], dtype=torch.float64)\n",
      "[5] feature tensor([[6.7000, 3.3000, 5.7000, 2.5000],\n",
      "        [6.4000, 2.7000, 5.3000, 1.9000],\n",
      "        [6.0000, 3.4000, 4.5000, 1.6000],\n",
      "        [6.3000, 2.9000, 5.6000, 1.8000],\n",
      "        [6.4000, 3.1000, 5.5000, 1.8000]], dtype=torch.float64)\n",
      "[6] feature tensor([[5.0000, 3.0000, 1.6000, 0.2000],\n",
      "        [5.6000, 3.0000, 4.5000, 1.5000],\n",
      "        [5.2000, 2.7000, 3.9000, 1.4000],\n",
      "        [6.1000, 2.8000, 4.7000, 1.2000],\n",
      "        [5.8000, 4.0000, 1.2000, 0.2000]], dtype=torch.float64)\n",
      "[7] feature tensor([[5.0000, 3.4000, 1.6000, 0.4000],\n",
      "        [6.7000, 3.1000, 4.4000, 1.4000],\n",
      "        [6.8000, 3.2000, 5.9000, 2.3000],\n",
      "        [5.0000, 3.6000, 1.4000, 0.2000],\n",
      "        [6.2000, 2.2000, 4.5000, 1.5000]], dtype=torch.float64)\n",
      "[8] feature tensor([[4.7000, 3.2000, 1.3000, 0.2000],\n",
      "        [5.5000, 4.2000, 1.4000, 0.2000],\n",
      "        [4.9000, 3.1000, 1.5000, 0.1000],\n",
      "        [5.8000, 2.7000, 5.1000, 1.9000],\n",
      "        [7.2000, 3.0000, 5.8000, 1.6000]], dtype=torch.float64)\n",
      "[9] feature tensor([[7.3000, 2.9000, 6.3000, 1.8000],\n",
      "        [5.1000, 3.8000, 1.6000, 0.2000],\n",
      "        [4.6000, 3.4000, 1.4000, 0.3000],\n",
      "        [4.8000, 3.0000, 1.4000, 0.1000],\n",
      "        [4.9000, 3.1000, 1.5000, 0.1000]], dtype=torch.float64)\n",
      "[10] feature tensor([[5.6000, 2.7000, 4.2000, 1.3000],\n",
      "        [7.6000, 3.0000, 6.6000, 2.1000],\n",
      "        [7.7000, 3.0000, 6.1000, 2.3000],\n",
      "        [7.0000, 3.2000, 4.7000, 1.4000],\n",
      "        [5.0000, 3.5000, 1.3000, 0.3000]], dtype=torch.float64)\n",
      "[11] feature tensor([[5.0000, 3.3000, 1.4000, 0.2000],\n",
      "        [6.0000, 3.0000, 4.8000, 1.8000],\n",
      "        [5.1000, 3.8000, 1.5000, 0.3000],\n",
      "        [7.9000, 3.8000, 6.4000, 2.0000],\n",
      "        [4.7000, 3.2000, 1.6000, 0.2000]], dtype=torch.float64)\n",
      "[12] feature tensor([[5.0000, 2.0000, 3.5000, 1.0000],\n",
      "        [6.9000, 3.2000, 5.7000, 2.3000],\n",
      "        [6.6000, 3.0000, 4.4000, 1.4000],\n",
      "        [5.1000, 3.7000, 1.5000, 0.4000],\n",
      "        [5.0000, 3.5000, 1.6000, 0.6000]], dtype=torch.float64)\n",
      "[13] feature tensor([[5.5000, 2.3000, 4.0000, 1.3000],\n",
      "        [6.7000, 3.0000, 5.2000, 2.3000],\n",
      "        [4.5000, 2.3000, 1.3000, 0.3000],\n",
      "        [5.1000, 3.5000, 1.4000, 0.3000],\n",
      "        [6.2000, 3.4000, 5.4000, 2.3000]], dtype=torch.float64)\n",
      "[14] feature tensor([[5.7000, 2.5000, 5.0000, 2.0000],\n",
      "        [6.1000, 2.9000, 4.7000, 1.4000],\n",
      "        [6.1000, 3.0000, 4.9000, 1.8000],\n",
      "        [6.4000, 2.9000, 4.3000, 1.3000],\n",
      "        [5.4000, 3.9000, 1.3000, 0.4000]], dtype=torch.float64)\n",
      "[15] feature tensor([[6.4000, 3.2000, 5.3000, 2.3000],\n",
      "        [5.7000, 3.8000, 1.7000, 0.3000],\n",
      "        [6.5000, 3.2000, 5.1000, 2.0000],\n",
      "        [5.8000, 2.7000, 4.1000, 1.0000],\n",
      "        [6.3000, 2.7000, 4.9000, 1.8000]], dtype=torch.float64)\n",
      "[16] feature tensor([[6.0000, 2.7000, 5.1000, 1.6000],\n",
      "        [6.2000, 2.9000, 4.3000, 1.3000],\n",
      "        [5.7000, 2.8000, 4.5000, 1.3000],\n",
      "        [5.4000, 3.7000, 1.5000, 0.2000],\n",
      "        [5.7000, 3.0000, 4.2000, 1.2000]], dtype=torch.float64)\n",
      "[17] feature tensor([[7.2000, 3.2000, 6.0000, 1.8000],\n",
      "        [7.2000, 3.6000, 6.1000, 2.5000],\n",
      "        [6.3000, 3.4000, 5.6000, 2.4000],\n",
      "        [6.5000, 3.0000, 5.8000, 2.2000],\n",
      "        [5.7000, 2.8000, 4.1000, 1.3000]], dtype=torch.float64)\n",
      "[18] feature tensor([[5.5000, 2.4000, 3.8000, 1.1000],\n",
      "        [5.2000, 3.5000, 1.5000, 0.2000],\n",
      "        [5.2000, 4.1000, 1.5000, 0.1000],\n",
      "        [6.3000, 2.5000, 4.9000, 1.5000],\n",
      "        [5.0000, 3.4000, 1.5000, 0.2000]], dtype=torch.float64)\n",
      "[19] feature tensor([[5.6000, 2.8000, 4.9000, 2.0000],\n",
      "        [6.9000, 3.1000, 5.1000, 2.3000],\n",
      "        [5.1000, 3.3000, 1.7000, 0.5000],\n",
      "        [6.8000, 2.8000, 4.8000, 1.4000],\n",
      "        [4.8000, 3.1000, 1.6000, 0.2000]], dtype=torch.float64)\n",
      "[20] feature tensor([[4.9000, 3.1000, 1.5000, 0.1000],\n",
      "        [6.7000, 2.5000, 5.8000, 1.8000],\n",
      "        [5.6000, 2.9000, 3.6000, 1.3000],\n",
      "        [7.7000, 2.8000, 6.7000, 2.0000],\n",
      "        [6.3000, 2.8000, 5.1000, 1.5000]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "for _, (feature,target) in enumerate(trainDF):\n",
    "    print(f'[{_}] feature {feature}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class model(nn.Module) :\n",
    "    def __init__(self):\n",
    "        super(model, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "model=model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실행 디바이스, 모델, 최적화, 손실함수, 학습횟수 \n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# 학습 횟수\n",
    "EPOCHS = 50\n",
    "LOSS_FN=nn.CrossEntropyLoss().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 학습 진행함수\n",
    "def training():\n",
    "    model.train() # 학습모드로 설정 => 정규화, 경사하강법, 드랍아웃 등의 기능 활성화 \n",
    "\n",
    "    for cnt, (feature, target) in enumerate(trainDF):\n",
    "        # 모델 학습\n",
    "        train_loss=[]\n",
    "        feature, target = feature.to(DEVICE), target.to(DEVICE)\n",
    "        pre_target = model(feature.float())\n",
    "        target = target.squeeze()\n",
    "        \n",
    "\n",
    "\n",
    "        loss=LOSS_FN(pre_target.argmax(dim=1).float(), target.float())\n",
    "        train_loss.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'[Train loss]==> {loss}')\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[167], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtraining\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[166], line 16\u001b[0m, in \u001b[0;36mtraining\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     train_loss\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     15\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 16\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[Train loss]==> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_38/lib/python3.8/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_38/lib/python3.8/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex=torch.tensor([[-3.3297, -3.5757, -2.4487],\n",
    "        [-1.2773, -2.1167, -1.1149],\n",
    "        [-3.4242, -4.0081, -2.4938],\n",
    "        [-1.9770, -2.5509, -1.3195],\n",
    "        [-3.0342, -3.1301, -2.3086]])\n",
    "ex.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing():\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict():\n",
    "    pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for eps in range(EPOCHS):\n",
    "    training()\n",
    "    testing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "criterion=nn.MSELoss()\n",
    "for epoch in range(100):\n",
    "    for i, (feature,target) in enumerate(trainDF):\n",
    "        y_pre=model(feature.float())\n",
    "\n",
    "        loss=criterion(y_pre, target.float())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
